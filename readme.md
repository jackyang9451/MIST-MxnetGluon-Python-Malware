# 1 概述
我主要以MIST数据集，通过神经网络的方法使用Mxnet/Gluon开发一种恶意程序的分类识别方法，程序设计语言为Python。

# 2 数据集
MIST数据集的下载地址为：https://github.com/marcoramilli/MalwareTrainingSets

# 3 特征提取
通常恶意程序的名称都包含APT、Crypto、Locker和Zeus这样的关键字，为了避免程序名称对分类算法的干扰，我们会用空格来替换这些关键字，这种替换是忽略大小写的，所以会使用re.I：
```
def load_files_from_dir(dir):
    import glob
    files=glob.glob(dir)
    result = []
    for file in files:
        #print "Load file %s" % file
        with open(file) as f:
            lines=f.readlines()
            lines_to_line=" ".join(lines)
            lines_to_line = re.sub(r"[APT|Crypto|Locker|Zeus]", ' ', lines_to_line,flags=re.I)
            result.append(lines_to_line)
    return result
```
遍历APT、Crypto、Locker和Zeus4个文件夹下的文件，分别给这4个文件夹下的文件分配对应标签1～4，以对应不同的恶意程序分类：
```
def load_files():
    #/data/malware/MalwareTrainingSets-master/trainingSets
    malware_class=['APT1','Crypto','Locker','Zeus']
    x=[]
    y=[]
    for i,family in enumerate(malware_class):
        dir="../MalwareTrainingSets-master/trainingSets/%s/*" % family
        print('Load files from %s index %d' % (dir,i))
        v=load_files_from_dir(dir)
        x+=v
        y+=[i]*len(v)
    print('Loaded files %d' % len(x))
    return x,y
```

# 4 预处理文本
我们需要对每条软件信息做分词，从而得到分好词的结果。这里定义的get_tokenized_imdb函数使用最简单的方法：基于空格进行分词。
```
def get_tokenized_imdb(x_data):
    def tokenizer(text):
        return [tok for tok in text.split(' ')]
    return [tokenizer(review) for review in x_data]
```
现在，我们可以根据分好词的训练数据集来创建词典了。
```
def get_vocab_imdb(x_data):  
    tokenized_data = get_tokenized_imdb(x_data)
    counter = collections.Counter([tk for st in tokenized_data for tk in st])
    return text.vocab.Vocabulary(counter)
```
因为每条信息长度不一致所以不能直接组合成小批量，我们定义preprocess_imdb函数对每条信息进行分词，并通过词典转换成词索引，然后通过截断或者补0来将每条评论长度固定成10000。
```
def preprocess_imdb(x_data, vocab, y_data):  
    max_l = 10000  

    def pad(x):
        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))

    tokenized_data = get_tokenized_imdb(x_data)
    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])
    labels = nd.array([score for score in y_data])
    return features, labels
```
# 5 创建数据迭代器
现在，我们创建数据迭代器。每次迭代将返回一个小批量的数据。
```
batch_size = 20
train_set = gdata.ArrayDataset(*preprocess_imdb(x_train, vocab1, y_train))
test_set = gdata.ArrayDataset(*preprocess_imdb(x_test, vocab2, y_test))
train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)
test_iter = gdata.DataLoader(test_set, batch_size)
```
打印第一个小批量数据的形状以及训练集中小批量的个数。
```
for X, y in train_iter:
    print('X', X.shape, 'y', y.shape)
    break
'#batches:', len(train_iter)
```
# 6 多层感知机
## 6.1 初始化模型参数
```
num_outputs = 4
num_hidden = 64
net = gluon.nn.Sequential()
with net.name_scope():
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(num_outputs))
```
## 6.2 对参数进行初始化
```
net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)
```
## 6.3 Softmax交叉熵损失
```
softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()
```
## 6.4 Optimizer
trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .01})
## 6.5 评价指标
```
def evaluate_accuracy(data_iterator, net):
    acc = mx.metric.Accuracy()
    for i, (data, label) in enumerate(data_iterator):
        data = data.as_in_context(model_ctx).reshape((-1, 10000))
        label = label.as_in_context(model_ctx)
        output = net(data)
        predictions = nd.argmax(output, axis=1)
        acc.update(preds=predictions, labels=label)
    return acc.get()[1]
```
## 6.6 训练循环
```
epochs = 5
smoothing_constant = 0.1
for e in range(epochs):
    cumulative_loss = 0
    for i, (data, label) in enumerate(train_iter):
        data = data.as_in_context(model_ctx).reshape((-1, 10000))
        label = label.as_in_context(model_ctx)
        with autograd.record():
            output = net(data)
            loss = softmax_cross_entropy(output, label)
        loss.backward()
        trainer.step(data.shape[0])
        cumulative_loss += nd.sum(loss).asscalar()

    train_accuracy = evaluate_accuracy(train_iter, net)
    test_accuracy = evaluate_accuracy(test_iter, net)
    print('-------------------------')
    print("Loop: %s.\nLoss: %s\nTrain_acc %s\nTest_acc %s\n" %
          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))
```