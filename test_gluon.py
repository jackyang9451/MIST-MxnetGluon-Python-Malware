#%%
from __future__ import print_function
import numpy as np
import mxnet as mx
from mxnet import nd, autograd, gluon
import collections
from mxnet.contrib import text
from mxnet.gluon import data as gdata
import re
def load_files_from_dir(dir):
    import glob
    files=glob.glob(dir)
    result = []
    for file in files:
        #print "Load file %s" % file
        with open(file) as f:
            lines=f.readlines()
            lines_to_line=" ".join(lines)
            lines_to_line = re.sub(r"[APT|Crypto|Locker|Zeus]", ' ', lines_to_line,flags=re.I)
            result.append(lines_to_line)
    return result

def load_files():
    #/data/malware/MalwareTrainingSets-master/trainingSets
    malware_class=['APT1','Crypto','Locker','Zeus']
    x=[]
    y=[]
    for i,family in enumerate(malware_class):
        dir="../MalwareTrainingSets-master/trainingSets/%s/*" % family
        print('Load files from %s index %d' % (dir,i))
        v=load_files_from_dir(dir)
        x+=v
        y+=[i]*len(v)
    print('Loaded files %d' % len(x))
    return x,y

ctx = mx.gpu() if mx.test_utils.list_gpus() else mx.cpu()
data_ctx = ctx
model_ctx = ctx

num_examples = 4762
x, y = load_files()
n = int(len(x) / 2)
x_train, y_train = x[:n], y[:n]
x_test, y_test = x[n:], y[n:]



def get_tokenized_imdb(x_data):
    def tokenizer(text):
        return [tok for tok in text.split(' ')]
    return [tokenizer(review) for review in x_data]


def get_vocab_imdb(x_data):  
    tokenized_data = get_tokenized_imdb(x_data)
    counter = collections.Counter([tk for st in tokenized_data for tk in st])
    return text.vocab.Vocabulary(counter)

def preprocess_imdb(x_data, vocab, y_data):  
    max_l = 10000  

    def pad(x):
        return x[:max_l] if len(x) > max_l else x + [0] * (max_l - len(x))

    tokenized_data = get_tokenized_imdb(x_data)
    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])
    labels = nd.array([score for score in y_data])
    return features, labels

vocab1 = get_vocab_imdb(x_train)
vocab2 = get_vocab_imdb(x_test)

batch_size = 20
train_set = gdata.ArrayDataset(*preprocess_imdb(x_train, vocab1, y_train))
test_set = gdata.ArrayDataset(*preprocess_imdb(x_test, vocab2, y_test))
train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)
test_iter = gdata.DataLoader(test_set, batch_size)

for X, y in train_iter:
    print('X', X.shape, 'y', y.shape)
    break
'#batches:', len(train_iter)
# def transform(data, label):
#     return data.astype(np.float32)/255, label.astype(np.float32)

# train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),
#                                       batch_size, shuffle=True)
# test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),
#                                      batch_size, shuffle=False)
num_outputs = 4
num_hidden = 64
net = gluon.nn.Sequential()
with net.name_scope():
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(num_hidden, activation="relu"))
    net.add(gluon.nn.Dense(num_outputs))


net.collect_params().initialize(mx.init.Normal(sigma=.1), ctx=model_ctx)

softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()

trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})

def evaluate_accuracy(data_iterator, net):
    acc = mx.metric.Accuracy()
    for i, (data, label) in enumerate(data_iterator):
        data = data.as_in_context(model_ctx).reshape((-1, 10000))
        label = label.as_in_context(model_ctx)
        output = net(data)
        predictions = nd.argmax(output, axis=1)
        acc.update(preds=predictions, labels=label)
    return acc.get()[1]

epochs = 15
smoothing_constant = 0.1
for e in range(epochs):
    cumulative_loss = 0
    for i, (data, label) in enumerate(train_iter):
        data = data.as_in_context(model_ctx).reshape((-1, 10000))
        label = label.as_in_context(model_ctx)
        with autograd.record():
            output = net(data)
            loss = softmax_cross_entropy(output, label)
        loss.backward()
        trainer.step(data.shape[0])
        cumulative_loss += nd.sum(loss).asscalar()


    train_accuracy = evaluate_accuracy(train_iter, net)
    test_accuracy = evaluate_accuracy(test_iter, net)
    print('-------------------------')
    print("Loop: %s.\nLoss: %s\nTrain_acc %s\nTest_acc %s\n" %
          (e, cumulative_loss/num_examples, train_accuracy, test_accuracy))
